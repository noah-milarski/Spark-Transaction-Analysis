# -*- coding: utf-8 -*-
"""TDE3-Finalized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/166MjdLzN8mx8XykXqCzn3mV5IbXeTLVJ
"""

# Install libraries
#!pip install pyspark

# Import libraries
from pyspark.sql import SparkSession
from pyspark import SparkFiles
import pyspark.sql.functions as func

# Create Spark context
spark = SparkSession.builder.appName("CommercialOperationsAnalysis").getOrCreate()

# Load the CSV file with the full path
df = spark.read.option("header", "true").option("delimiter", ";").csv("/your_path/complete_comercial_operations.csv")

# Handle missing values
df = df.fillna({
    "trade_usd": 0,
    "country_or_area": "None",
    "year": 0,
    "comm_code": 0,
    "commodity": None,
    "flow": None,
    "weight_kg": 0,
    "quantity_name": None,
    "quantity": 0,
    "category": None
})

# Show the first few rows to verify the data load
df.show(5)

# Convert DataFrame to RDD
rdd = df.rdd

# 1. Number of transactions involving Brazil using RDD. The return should be a single value.
transactions_brazil = rdd.filter(lambda row: row["country_or_area"] == "Brazil").count()
print(transactions_brazil)

# EXPLANATION
# - `df.rdd` converts the DataFrame `df` into an RDD, a fundamental data structure in Spark for distributed processing.
#   This allows the use of operations like `filter` and `count` directly on the dataset.
#
# - `filter(lambda row: row["country_or_area"] == "Brazil")` is a transformation that applies a lambda function to each row
#   of the RDD. This transformation creates a new RDD that includes only rows where the "country_or_area" field is "Brazil".
#   In other words, we are filtering the data to include only transactions related to Brazil.
#
# - `.count()` is an action that counts the number of elements (rows) in the filtered RDD. This triggers the execution of the 
#   transformation pipeline and returns the total number of transactions involving Brazil.



# 2. Number of transactions involving Brazil using PairRDD
transactions_brazil_pair_rdd = rdd.filter(lambda row: row["country_or_area"] == "Brazil").map(lambda row: ("Brazil", 1)).reduceByKey(lambda a, b: a + b)
print(transactions_brazil_pair_rdd.collect())

# EXPLANATION
# - `filter(lambda row: row["country_or_area"] == "Brazil")`: This is a transformation that creates a new RDD, including only the rows
#   where the "country_or_area" field is "Brazil". In other words, we're filtering for transactions related to Brazil.
#
# - `map(lambda row: ("Brazil", 1))`: This transformation converts each filtered row into a key-value pair (PairRDD), where the key is the 
#   string "Brazil" and the value is `1`. Each transaction involving Brazil is represented by this pair, making it easy to count using aggregation operations.
#
# - `reduceByKey(lambda a, b: a + b)`: `reduceByKey` is an operation that groups the pairs by key ("Brazil") and applies the function `lambda a, b: a + b`
#   to sum all the values of transactions with the same key (in this case, Brazil). This way, Spark sums all the `1`s to get the total number of transactions.
#
# - `.collect()`: This action collects the count results (in this case, a list containing the pair ("Brazil", transaction_count)) and returns them to the driver.
#   `collect` is used here to display the final result.
#
# - `print(transactions_brazil_pair_rdd.collect())`: Displays the result of the count in the form of a list with the key-value pair, such as:
#   `[("Brazil", 123)]`, where `123` would be the total number of transactions involving Brazil.



# 3. Number of transactions involving Brazil in 2016 using RDD
transactions_brazil_2016 = rdd.filter(lambda row: row["country_or_area"] == "Brazil" and row["year"] == "2016").count()
print(transactions_brazil_2016)

# EXPLANATION
# - `filter(lambda row: row["country_or_area"] == "Brazil" and row["year"] == "2016")`: This transformation applies a filter to select only the rows in the RDD
#   where the "country_or_area" field is "Brazil" and the "year" field is "2016". Thus, we are filtering the data to include only transactions that occurred in Brazil
#   during 2016.
#
# - `.count()`: `count()` is an action that counts the number of elements in the filtered RDD (i.e., the number of transactions meeting the criteria
#   of country and year specified). The `count()` operation triggers the execution of all transformations in the pipeline.
#   The final value returned will be the total number of transactions in Brazil during 2016.
#
# - `print(transactions_brazil_2016)`: Displays the result, which will be a single integer representing the total number of transactions involving Brazil in 2016.



# 4. Number of transactions involving Brazil in 2016 using PairRDD
transactions_brazil_2016_pair_rdd = rdd.filter(lambda row: row["country_or_area"] == "Brazil" and row["year"] == "2016").map(lambda row: (("Brazil", "2016"), 1)).reduceByKey(lambda a, b: a + b)
print(transactions_brazil_2016_pair_rdd.collect())

# EXPLANATION
# - `filter(lambda row: row["country_or_area"] == "Brazil" and row["year"] == "2016")`: This is a transformation that filters the RDD to include only rows
#   where the "country_or_area" field is "Brazil" and the "year" field is "2016". This filter ensures that we are only analyzing transactions for Brazil
#   that occurred in 2016.
#
# - `map(lambda row: (("Brazil", "2016"), 1))`: This transformation creates a PairRDD where each transaction is represented by the pair (key, value).
#   The key here is the tuple `("Brazil", "2016")`, and the value is `1`. This makes it easier to count the transactions using aggregation operations.
#
# - `reduceByKey(lambda a, b: a + b)`: `reduceByKey` groups the pairs by the key `("Brazil", "2016")` and sums the values `1` to obtain the total number of transactions
#   for this specific key. It performs the final count by summing all the `1`s associated with the key `("Brazil", "2016")`.
#
# - `.collect()`: `collect()` is an action that retrieves all the count results as a list and sends them back to the driver. The result will be a list containing the
#   key-value pair `[("Brazil", "2016"), transaction_count)]`, where `transaction_count` represents the total number of transactions for Brazil in 2016.
#
# - `print(transactions_brazil_2016_pair_rdd.collect())`: Displays the final result in the form of a list with the key-value pair, such as:
#   `[("Brazil", "2016"), 123)]`, where `123` represents the total number of transactions in Brazil during 2016.



# 5. Number of transactions per flow and year, starting from 2010, with flow in uppercase
transactions_per_year_flow = rdd.filter(lambda row: int(row["year"]) >= 2010).map(lambda row: ((row["year"], row["flow"].upper()), 1)).reduceByKey(lambda a, b: a + b).sortByKey()
print(transactions_per_year_flow.collect())

# EXPLANATION
# - `filter(lambda row: int(row["year"]) >= 2010)`: The `filter` transformation selects only the rows where the year (`year`) is greater than or equal to 2010.
#   The function `int(row["year"])` is used to ensure that the "year" field is treated as an integer.
#   This filter ensures that we are analyzing transactions from 2010 onwards.
#
# - `map(lambda row: ((row["year"], row["flow"].upper()), 1))`: The `map` transformation converts each row in the RDD into a key-value pair (PairRDD). The key here is a tuple
#   consisting of the year (`row["year"]`) and the flow (`row["flow"]`), with the flow being converted to uppercase using `.upper()`.
#   The value associated with each key-value pair is `1`, representing a single transaction.
#   The use of `.upper()` ensures that all flows are normalized to uppercase for counting purposes.
#
# - `reduceByKey(lambda a, b: a + b)`: The `reduceByKey` operation groups the pairs by the key, which is the tuple `(year, flow in uppercase)`. Then it applies the function
#   `lambda a, b: a + b` to sum all the transaction counts for each unique key (i.e., each unique combination of year and flow).
#
# - `sortByKey()`: This operation sorts the results by the key (year and flow) in ascending order. This allows us to see the transaction counts for each year and flow, ordered
#   by year first and then by flow.
#
# - `.collect()`: The `collect()` action gathers the results as a list and returns them to the driver.
#   The result is a list of tuples, where each tuple contains a key (the year and flow in uppercase) and the corresponding transaction count for that year-flow pair.
#
# - `print(transactions_per_year_flow.collect())`: Displays the result, which will be a list of tuples like:
#   `[("2010", "EXPORT"), 123], ("2010", "IMPORT"), 200], ...]`, showing the total number of transactions for each year and flow combination.



# 6. Number of transactions per commodity, per country, and per year, with only year >= 2015
transactions_per_commodity_country_year = rdd.filter(lambda row: int(row["year"]) >= 2015).map(lambda row: ((row["year"], row["country_or_area"], row["commodity"]), 1)).reduceByKey(lambda a, b: a + b).sortByKey()
print(transactions_per_commodity_country_year.collect())

# EXPLANATION
# - `filter(lambda row: int(row["year"]) >= 2015)`: This transformation filters the rows to include only those where the "year" field is greater than or equal to 2015.
#   This ensures we are working with transactions that occurred from 2015 onwards.
#
# - `map(lambda row: ((row["year"], row["country_or_area"], row["commodity"]), 1))`: This transformation creates a PairRDD where each row is converted into a tuple key:
#   `(year, country, commodity)`. The value associated with each key is `1`, representing a single transaction. This allows us to count the number of transactions
#   per unique combination of year, country, and commodity.
#
# - `reduceByKey(lambda a, b: a + b)`: The `reduceByKey` operation groups the pairs by the key `(year, country, commodity)`, and sums the values (which are all `1`s)
#   to count the total number of transactions for each specific combination of year, country, and commodity.
#
# - `sortByKey()`: This sorts the results by the key (year, country, commodity). The sorting is done in ascending order, first by year, then by country, and finally by commodity.
#
# - `.collect()`: This action collects the results as a list and sends them back to the driver. The result will be a list of tuples, where each tuple consists of a key-value pair,
#   with the key being `(year, country, commodity)` and the value being the total transaction count for that combination.
#
# - `print(transactions_per_commodity_country_year.collect())`: Displays the result, which will look something like:
#   `[("2015", "Brazil", "Coffee"), 120], ("2015", "Argentina", "Soy"), 150], ...]`, showing the total number of transactions for each year, country, and commodity.



# 7. Summary of total trade value per country or area, sorted in descending order by trade value
total_trade_value_per_country = df.groupBy("country_or_area").agg(func.sum("trade_usd").alias("total_trade_value")).sort(func.col("total_trade_value").desc())
total_trade_value_per_country.show()

# EXPLANATION
# - `df.groupBy("country_or_area")`: This groups the DataFrame by the "country_or_area" column, creating a new grouping for each unique country or area.
#
# - `agg(func.sum("trade_usd").alias("total_trade_value"))`: The `agg()` function allows us to perform an aggregation on each group. In this case, we are calculating the sum of the "trade_usd" column,
#   which represents the trade value in USD, for each country or area. The result is aliased as "total_trade_value".
#
# - `sort(func.col("total_trade_value").desc())`: This sorts the results by the "total_trade_value" column in descending order, ensuring that the countries with the highest trade values appear first.
#
# - `total_trade_value_per_country.show()`: Displays the final result, showing each country/area along with its corresponding total trade value in USD, sorted in descending order.



# 8. Summary of the number of transactions by category, only considering rows where the category is not None
transactions_by_category = df.filter(df["category"].isNotNull()).groupBy("category").agg(func.count("*").alias("transaction_count")).sort(func.col("transaction_count").desc())
transactions_by_category.show()

# EXPLANATION
# - `df.filter(df["category"].isNotNull())`: This filters the DataFrame to include only rows where the "category" column is not null. This ensures that we are analyzing only the transactions
#   that have a valid category.
#
# - `groupBy("category")`: This groups the data by the "category" column, so that each group contains all transactions of the same category.
#
# - `agg(func.count("*").alias("transaction_count"))`: The `agg()` function is used here to count the number of rows (transactions) in each category. The result is aliased as "transaction_count".
#
# - `sort(func.col("transaction_count").desc())`: This sorts the categories in descending order of the transaction count, so that the categories with the highest number of transactions
#   appear first.
#
# - `transactions_by_category.show()`: Displays the result, showing each category along with the total number of transactions in that category, sorted by transaction count in descending order.



# 9. Summary of the total weight of transactions per commodity, filtered to include only commodities with a total weight > 100,000 kg
total_weight_per_commodity = df.groupBy("commodity").agg(func.sum("weight_kg").alias("total_weight")).filter(func.col("total_weight") > 100000).sort(func.col("total_weight").desc())
total_weight_per_commodity.show()

# EXPLANATION
# - `df.groupBy("commodity")`: This groups the data by the "commodity" column, so that each group contains all transactions for the same commodity.
#
# - `agg(func.sum("weight_kg").alias("total_weight"))`: The `agg()` function calculates the sum of the "weight_kg" column for each commodity, resulting in the total weight of all transactions for each commodity.
#   The result is aliased as "total_weight".
#
# - `filter(func.col("total_weight") > 100000)`: This filters the results to include only commodities with a total weight greater than 100,000 kg.
#
# - `sort(func.col("total_weight").desc())`: This sorts the commodities by their total weight in descending order, so that the commodities with the highest total weight appear first.
#
# - `total_weight_per_commodity.show()`: Displays the final result, showing each commodity and its total weight, but only for those commodities with a total weight greater than 100,000 kg.



# 10. Average trade value per commodity, grouped by year
average_trade_value_per_commodity_year = df.groupBy("year", "commodity").agg(func.avg("trade_usd").alias("avg_trade_value")).sort("year", "commodity")
average_trade_value_per_commodity_year.show()

# EXPLANATION:
# - `df.groupBy("year", "commodity")`: This groups the data first by year and then by commodity. Each group represents transactions of a specific commodity for a particular year.
#
# - `agg(func.avg("trade_usd").alias("avg_trade_value"))`: The `agg()` function calculates the average trade value (in USD) for each group (combination of year and commodity).
#   This results in the average trade value per commodity for each year. The result is aliased as "avg_trade_value".
#
# - `sort("year", "commodity")`: This sorts the results first by year and then by commodity, allowing you to see how the average trade value evolves for each commodity over time.
#
# - `average_trade_value_per_commodity_year.show()`: Displays the results, showing the average trade value per commodity for each year.



# 11. Transaction count per continent
transactions_by_continent = df.groupBy("continent").agg(func.count("*").alias("transaction_count")).sort(func.col("transaction_count").desc())
transactions_by_continent.show()

# EXPLANATION:
# - `df.groupBy("continent")`: This groups the data by the "continent" column, so that all transactions within each continent are grouped together.
#
# - `agg(func.count("*").alias("transaction_count"))`: The `agg()` function counts the number of rows (transactions) for each continent, giving us the total number of transactions for each continent.
#   The result is aliased as "transaction_count".
#
# - `sort(func.col("transaction_count").desc())`: The results are sorted in descending order of the transaction count, so that continents with the highest number of transactions appear first.
#
# - `transactions_by_continent.show()`: Displays the final result, showing each continent along with the total transaction count in descending order.



# 12. Top 10 commodities by total trade value (descending)
top_commodities_by_value = df.groupBy("commodity").agg(func.sum("trade_usd").alias("total_trade_value")).sort(func.col("total_trade_value").desc()).limit(10)
top_commodities_by_value.show()

# EXPLANATION:
# - `df.groupBy("commodity")`: This groups the data by commodity, aggregating all transactions involving the same commodity.
#
# - `agg(func.sum("trade_usd").alias("total_trade_value"))`: The `agg()` function calculates the sum of the trade values (`trade_usd`) for each commodity, resulting in the total trade value for each commodity.
#   This value is aliased as "total_trade_value".
#
# - `sort(func.col("total_trade_value").desc())`: The results are sorted in descending order of the total trade value, so that commodities with the highest trade value appear first.
#
# - `limit(10)`: The `limit(10)` function restricts the result to the top 10 commodities with the highest trade value.
#
# - `top_commodities_by_value.show()`: Displays the top 10 commodities along with their total trade value.